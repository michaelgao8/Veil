import os
import argparse
import yaml
from collections import OrderedDict

parser = argparse.ArgumentParser(description='Generate initial YAML file to start deidentification')
parser.add_argument("-i", "--input", type = str, default="./",
                    help="Directory that contains the input files")
parser.add_argument("-o", "--output", type = str, default = "./veil.yml",
                    help="Name of configuration file to write")
args = parser.parse_args()


def represent_ordereddict(dumper, data):
    value = []

    for item_key, item_value in data.items():
        node_key = dumper.represent_data(item_key)
        node_value = dumper.represent_data(item_value)

        value.append((node_key, node_value))

    return yaml.nodes.MappingNode(u'tag:yaml.org,2002:map', value)


if __name__ == '__main__':
    yaml.add_representer(OrderedDict, represent_ordereddict) #https://stackoverflow.com/questions/16782112/can-pyyaml-dump-dict-items-in-non-alphabetical-order

    # First, begin by creating a file and writing it:
    with open(args.output, 'w+') as f:
        f.write("""##### Initial Configuration generated by Veil.py #####

## Background:
# As part of the process of using Veil to de-identify files, you as the user 
# will need to specify some parameters so that Veil can figure out the best 
# way to de-deidentify your data. Any field that is marked #TODO# needs to
# be completed by the user before this configuration is valid

### Time Offset Method ###
# There are two ways of offsetting dates in Veil. Random takes a random number
# of days, and shifts all dates in a dataset by that amount. You can modify 
# the max days below. The year_start method will be implemented in a future 
# version of veil. 

## #TODO#  Please change `time_offset_method` to just random or year_start

time_offset_method: #TODO# random OR year_start
max_days: 365

### Column Aliases ###
# Occassionally, there will be files that have different column names
# that actually refer to the same thing. You will need to specify them 
# below. Here is an example:

# alias1:
#   - pat_id
#   - PATIENT_ID
#   - patient_id
#   - patient_num

# An important note: the name of the group MUST begin with the word alias,
# as this is how Veil finds it in this configuration:

# If you create your own alias column, please make sure that it is uncommented

###### Columns to Deidentify #######
# Below is an auto-generated list for every .csv file in this directory.
# After each filename are two lists, the first is a datetime list and 
# second is an ID list. Within those lists, ALL column names are currently
# populated. Please trim those down to only include the correct columns.
# Datetime columns should be any columns which will need to be time shifted
# ID columns are any columns which will need to be masked.

### Exclude ###
# There are some columns which cannot be reasonably deidentified.
# For example, addresses. You can specify which column to remove entirely
# from a dataset by specifying them under exclude.

### Datetime columns Initialization ###
# Datetime columns need to be initialized with an ID column. For example,
# You may choose to shift a date similarly for all rows with the same 
# patient ID. To specify the correct column to base the datetime shifts on,
# include it in the following configuration:
# NOTE: Valid values include a column name or an alias name. 


datetime_base: #TODO# for example, PATIENT_ID, or alias1

### COLUMNS ###
# Please trim down each list until only the correct columns
# are specified within each section

""")
        f.close()


    file_list = os.listdir(args.input)
    csv_list = [filename for filename in file_list if '.csv' in filename]

    filename_dict = {}
    for filename in csv_list:
        try:
            with open(str(args.input) + str(filename), 'r') as f:
                columns = f.readline().strip()
        except UnicodeDecodeError:
            with open(str(args.input) + str(filename), 'r', encoding = 'ISO-8859-1') as f:
                columns = f.readline().strip()
        
        column_list = columns.split(',')
        
        file_entry = OrderedDict([('id', column_list)])
        file_entry.update([('datetime', column_list)])
        file_entry.update([('exclude', column_list)])
        filename_dict[filename] = file_entry

    yaml.Dumper.ignore_aliases = lambda *args : True

    wrapper_dict = {'files': filename_dict}
    with open(args.output, 'a') as outfile:
        yaml.dump(wrapper_dict, outfile, default_flow_style=False)

    
